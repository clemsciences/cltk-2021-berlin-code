{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "073f9622",
   "metadata": {},
   "source": [
    "# About the discovery of America by Vikings with CLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f00233",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The aims of the presentation are to show:\n",
    "- to show how to practically use CLTK,\n",
    "- to understand how CLTK works,\n",
    "- to explore classical texts with CLTK on an exploratory perspective.\n",
    "\n",
    "We analyse here two texts *The saga of Erik the Red* (Eir√≠ks saga rau√∞a, or **ESR**) and *The saga of the Greenlanders* (Gr≈ìnlendinga saga or **GS**) in the original language, namely Old Norse. The sagas tells how Norwegians/Icelanders colonized Greenland and found new lands in what is now America.\n",
    "\n",
    "\n",
    "### Context\n",
    "\n",
    "#### Archeology\n",
    "The discovery of America by Norse people is now a fact. Archeological evidences show the presence of Vikings at *L‚ÄôAnse aux Meadows* in the current Canada. ([The Norse in Newfoundland](https://www.erudit.org/en/journals/nflds/2003-v19-n1-nflds_19_1/nflds19_1art02/) and [A Nature article: Evidence for European presence in the Americas in ad 1021](https://doi.org/10.1038/s41586-021-03972-8).\n",
    "\n",
    "#### Interpretations\n",
    "Scholars interpret **ESR** as a more logical story reinterpreted by the author whereas **GS** is supposedly more raw transcription of an oral tradition. There is a debate on which one came first.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083cb21d",
   "metadata": {},
   "source": [
    "## Installing CLTK "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe44782",
   "metadata": {},
   "source": [
    "It works on Posix system: **Linux**, **MacOs**, **Windows** with **WSL** (Windows subsystem for Linux)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecb5904",
   "metadata": {},
   "source": [
    "It is a good practice to code in a Python virtual environment. Create a virtual environment to work on a project.\n",
    "\n",
    "```bash\n",
    "$ python3 -m venv cltk_venv\n",
    "$ source cltk_venv/bin/activate\n",
    "(cltk_venv) $ pip install cltk \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3235b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfad9a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cltk 1.0.21\n"
     ]
    }
   ],
   "source": [
    "print(cltk.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86657ff5",
   "metadata": {},
   "source": [
    "## Text retrieval\n",
    "First, we need to retrieve the texts we want to analyse.\n",
    "\n",
    "The texts come from [heimskringla.no](http://heimskringla.no/wiki/Forside)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de324dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "def load_file(filename):\n",
    "    with codecs.open(filename, encoding=\"utf-8\") as f:\n",
    "        \n",
    "        return f.read()\n",
    "    \n",
    "def load_clean_text(filename):\n",
    "    text = load_file(filename)\n",
    "    text = text.replace('\"', \"\")\n",
    "    text = text.replace(\"-\", \" \")\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "ESR = load_clean_text(\"eiriks_saga_rauda.txt\")    \n",
    "GS = load_clean_text(\"gr√¶nlendiga_saga.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d619ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. fr√° au√∞i dj√∫p√∫√∞gu ok v√≠fli.\r\n",
      "\r\n",
      "√≥l√°fr h√©t herkonungr, er kalla√∞r var √≥l√°fr hv√≠ti. hann var sonr ingjalds konungs helgasonar, √≥l√°fssonar, gu√∞r√∂√∞arsonar, h√°lfdanarsonar hv√≠tbeins upplendingakonungs.\r\n",
      "√≥l√°fr herja√∞i √≠ vestrv√≠king ok vann dyflinni √° √≠rlandi ok dyflinnarsk√≠ri. √æar ger√∞ist hann konungr yfir. hann fekk au√∞ar dj√∫p√∫√∞gu, d√≥ttur ketils flatnefs, bjarnarsonar bunu, √°g√¶ts manns √≥r n√≥regi. √æorsteinn rau√∞r h√©t sonr √æeira.\r\n",
      "√≥l√°fr fell √° √≠rlandi √≠ orrostu, en au√∞r ok √æorsteinn f√≥ru √æ√° √≠ su√∞reyj\n",
      "\n",
      "===================\n",
      "\n",
      "1. fundit ok byggt gr√¶nland.\r\n",
      "\r\n",
      "√æorvaldr h√©t ma√∞r, sonr √°svalds √∫lfssonar, √∂xna √æ√≥rissonar. √æorvaldr ok eir√≠kr inn rau√∞i, sonr hans, f√≥ru af ja√∞ri til √≠slands fyrir v√≠ga sakir. √æ√° var v√≠√∞a byggt √≠sland. √æeir bjuggu fyrst at dr√∂ngum √° hornstr√∂ndum. √æar anda√∞ist √æorvaldr.\r\n",
      "eir√≠kr fekk √æ√° √æj√≥√∞hildar, d√≥ttur j√∂rundar √∫lfssonar ok √æorbjargar knarrarbringu, er √æ√° √°tti √æorbj√∂rn inn haukd√¶lski. r√©√∞st eir√≠kr √æ√° nor√∞an ok bj√≥ √° eir√≠ksst√∂√∞um hj√° vatnshorni. sonr eir√≠ks ok √æj√≥√∞hildar h√©t leifr.\r\n",
      "enn eftir v\n"
     ]
    }
   ],
   "source": [
    "print(f\"{ESR[:500]}\\n\\n===================\\n\\n{GS[:500]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74029fb6",
   "metadata": {},
   "source": [
    "## CLTK pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d58aa9",
   "metadata": {},
   "source": [
    "![CLTK pipeline](cltk-Page-2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0612e92",
   "metadata": {},
   "source": [
    "If everything is available in CLTK, you only need to import `NLP`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ce0d347",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk import NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef20fdf1",
   "metadata": {},
   "source": [
    "Default pipeline for Old Norse language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e119d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚Äéê§Ä CLTK version '1.0.21'.\n",
      "Pipeline for language 'Old Norse' (ISO: 'non'): `OldNorseTokenizationProcess`, `StopsProcess`, `OldNorseLexiconProcess`.\n"
     ]
    }
   ],
   "source": [
    "non_nlp_default = NLP(\"non\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e94de6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_nlp_default = NLP(\"non\", suppress_banner=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9ca464",
   "metadata": {},
   "source": [
    "Which processes are there? The answer is in the banner or in an attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "194a436f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[cltk.tokenizers.processes.OldNorseTokenizationProcess,\n",
       " cltk.stops.processes.StopsProcess,\n",
       " cltk.lexicon.processes.OldNorseLexiconProcess]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_nlp_default.pipeline.processes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80368360",
   "metadata": {},
   "source": [
    "A tokenization process for Old Norse, a stop process (a way to filter uninformative words) and a lexicon process. For our task, we do not need the lexicon process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9071ba",
   "metadata": {},
   "source": [
    "### Custom pipeline for Old Norse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69a8b30",
   "metadata": {},
   "source": [
    "To customize our pipeline, we have to import processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bbf5b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.tokenizers.processes import OldNorseTokenizationProcess\n",
    "from cltk.stops.processes import StopsProcess\n",
    "from cltk.text.processes import OldNorsePunctuationRemovalProcess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12118e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.core.data_types import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b089af99",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_pipeline_custom_1 = Pipeline(language=\"non\", description=\"\", processes=[OldNorseTokenizationProcess, StopsProcess])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1019952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚Äéê§Ä CLTK version '1.0.21'.\n",
      "Pipeline for language 'Old Norse' (ISO: 'non'): `OldNorseTokenizationProcess`, `StopsProcess`.\n"
     ]
    }
   ],
   "source": [
    "non_nlp_custom_1 = NLP(\"non\", custom_pipeline=non_pipeline_custom_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0b98f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "ESR_analysed_1 = non_nlp_custom_1.analyze(ESR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5a0e4d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 . fr√° au√∞i dj√∫p√∫√∞gu ok v√≠fli . √≥l√°fr h√©t herkonungr , er kalla√∞r var √≥l√°fr hv√≠ti . hann var sonr i'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ESR_analysed_1.sentences_strings[0][:100] # it does not work yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5763646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ESR_analysed_1.sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74689a44",
   "metadata": {},
   "source": [
    "Hmmm, 1 sentence means that sentences were not recognized.\n",
    "\n",
    "When available processed are not enough, you can create one by yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fb1a0c",
   "metadata": {},
   "source": [
    "###  Creating a `Process` for CLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a17d260",
   "metadata": {},
   "source": [
    "A process is a class that inherits from `Process` and that implements the `run` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b72b19d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from cltk.core import CLTKException\n",
    "from cltk.core.data_types import Doc, Process\n",
    "from cltk.sentence.sentence import RegexSentenceTokenizer\n",
    "\n",
    "non_sent_end_chars = [\".\", \"!\", \"?\"]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class OldNorseSentenceTokenizationProcess(Process):\n",
    "\n",
    "    model: object = None\n",
    "\n",
    "    def run(self, input_doc: Doc) -> Doc:\n",
    "        output_doc = deepcopy(input_doc)\n",
    "        sentence_tokenizer = RegexSentenceTokenizer(language=\"non\", sent_end_chars=non_sent_end_chars)\n",
    "\n",
    "        sentences = sentence_tokenizer.tokenize(output_doc.raw, self.model)\n",
    "        sentence_indices = []\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            if i >= 1:\n",
    "                sentence_indices.append(sentence_indices[-1] + len(sentences[i]))\n",
    "            else:\n",
    "                sentence_indices.append(len(sentence))\n",
    "        sentence_index = 0\n",
    "        for j, word in enumerate(output_doc.words):\n",
    "            if sentence_indices[sentence_index] < word.index_char_stop and\\\n",
    "                    sentence_index + 1 < len(sentence_indices):\n",
    "                sentence_index += 1\n",
    "            word.index_sentence = sentence_index\n",
    "        return output_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e25973d",
   "metadata": {},
   "source": [
    "In order to be more language-agnostic, we should add the `algorithm` method, then seperate the `run` method, that can stay in the mother class, from the `algorithm` method that is specific to each language and that is in a daughter class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588b6743",
   "metadata": {},
   "source": [
    "The sentence tokenizer, that can be used alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1dc5424",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.sentence.sentence import RegexSentenceTokenizer\n",
    "\n",
    "sent_end_chars = [\".\", \"!\", \"?\"]\n",
    "\n",
    "\n",
    "class OldNorseRegexSentenceTokenizer(RegexSentenceTokenizer):\n",
    "    \"\"\"``RegexSentenceTokenizer`` for Old Norse.\"\"\"\n",
    "\n",
    "    def __init__(self: object):\n",
    "        super().__init__(language=\"non\", sent_end_chars=sent_end_chars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bad1b9e",
   "metadata": {},
   "source": [
    "The global sentence tokenization process and the Old Norse one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c9e977d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from boltons.cacheutils import cachedproperty\n",
    "\n",
    "from cltk.core import CLTKException\n",
    "from cltk.core.data_types import Doc, Process\n",
    "from cltk.sentence.sentence import SentenceTokenizer\n",
    "\n",
    "@dataclass\n",
    "class SentenceTokenizationProcess(Process):\n",
    "\n",
    "    model: object = None\n",
    "\n",
    "    @cachedproperty\n",
    "    def algorithm(self):\n",
    "        raise CLTKException(f\"No sentence tokenization algorithm for language '{self.language}'.\")\n",
    "\n",
    "    def run(self, input_doc: Doc) -> Doc:\n",
    "        output_doc = deepcopy(input_doc)\n",
    "        sentence_tokenizer = self.algorithm\n",
    "        if not isinstance(sentence_tokenizer, SentenceTokenizer):\n",
    "            raise CLTKException(\"Algorithm must be an instance of SentenceTokenizer subclass\")\n",
    "\n",
    "        sentences = sentence_tokenizer.tokenize(output_doc.raw, self.model)\n",
    "        sentence_indices = []\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            if i >= 1:\n",
    "                sentence_indices.append(sentence_indices[-1] + len(sentences[i]))\n",
    "            else:\n",
    "                sentence_indices.append(len(sentence))\n",
    "        sentence_index = 0\n",
    "        for j, word in enumerate(output_doc.words):\n",
    "            if sentence_indices[sentence_index] < word.index_char_stop and\\\n",
    "                    sentence_index + 1 < len(sentence_indices):\n",
    "                sentence_index += 1\n",
    "            word.index_sentence = sentence_index\n",
    "        return output_doc\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class OldNorseSentenceTokenizationProcess(SentenceTokenizationProcess):\n",
    "\n",
    "    @cachedproperty\n",
    "    def algorithm(self):\n",
    "        return OldNorseRegexSentenceTokenizer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6335b1e1",
   "metadata": {},
   "source": [
    "We can now use it in our custom pipeline at the second position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e83075f",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_pipeline_2 = Pipeline(language=\"non\", description=\"\", \n",
    "                          processes=[OldNorseTokenizationProcess, \n",
    "                                     OldNorseSentenceTokenizationProcess,\n",
    "                                     OldNorsePunctuationRemovalProcess,\n",
    "                                     StopsProcess])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40790ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚Äéê§Ä CLTK version '1.0.21'.\n",
      "Pipeline for language 'Old Norse' (ISO: 'non'): `OldNorseTokenizationProcess`, `OldNorseSentenceTokenizationProcess`, `OldNorsePunctuationRemovalProcess`, `StopsProcess`.\n"
     ]
    }
   ],
   "source": [
    "non_nlp_2 = NLP(\"non\", custom_pipeline=non_pipeline_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc91e2b",
   "metadata": {},
   "source": [
    "Let's analyse our texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38c4b41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ESR_analysed_2 = non_nlp_2.analyze(ESR.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "edb56ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "GS_analysed_2 = non_nlp_2.analyze(GS.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26aaeaf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(587, 413)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ESR_analysed_2.sentences), len(GS_analysed_2.sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77600922",
   "metadata": {},
   "source": [
    "Out texts have the sentences we needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060e5174",
   "metadata": {},
   "source": [
    "## Characterisation of the topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d438382",
   "metadata": {},
   "source": [
    "The idea is to find differences in the user vocabulary. A non-factual vocabulary may show assumptions made by the authors. \n",
    "\n",
    "Some words are meaningful to see what it talks about whereas others are just here for syntactic reasons. The least-informative words are named *stop words*. There is a Stop Removal Process for Old Norse. It is however not complete enough. That is why I completed the list of stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "404b781d",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stop_list = [\"hann\", \"√æar\", \"hon\", \"af\", \"ek\", \"sv√°\", \"eigi\", \"n√∫\", \"haf√∞i\", \"honum\", \n",
    "                    \"hafa\", \"henni\", \"√æ√©r\", \"h√∂f√∞u\", \"mun\", \"hans\", \"s√©r\", \"eftir\", \"vera\", \"ekki\", \"m√©r\", \n",
    "                    \"√æ√∫\", \"aftr\", \"hana\", \"sitt\", \"haf\", \"v√©r\", \"s√≠num\", \"hennar\", \"s√≠nu\", \"√æa√∞an\", \n",
    "                    \"allt\", \"sinn\", \"hvat\", \"sama\", \"eitt\", \"einn\", \"ein\", \"√∂llum\", \"√∂√∞rum\", \"\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d6815c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_verbs = [\"var\", \"h√©t\", \"v√°ru\", \"f√≥r\", \"kom\", \"t√≥k\", \"fara\", \"m√¶lti\", \"sj√°\", \"kva√∞\", \"√æ√≥tti\", \"f√≥ru\", \n",
    "                \"√°tti\", \"sag√∞i\", \"bj√≥\", \"k√≥mu\", \"kve√∞st\", \"ver√∞a\", \"segir\", \"leita\", \"sigla\", \n",
    "                \"vil\", \"segja\", \"svarar\", \"gekk\", \"koma\", \"hefir\", \"t√≥ku\", \"v√¶ri\", \"vildi\", \"ger√∞ist\", \n",
    "                \"gera\", \"gaf\", \"k√∂llu√∞u\", \"kalla\", \"myndi\", \"mundu\", \"vildu\", \"gengu\", \"eru\", \"skal\", \"√¶tla\",\n",
    "                \"fekk\", \"l√©t\", \"svara√∞i\", \"√æ√≥ttust\", \"farit\", \"s√©\", \"vill\", \"sigldu\", \"vita\", \"taka\", \"mundi\", \n",
    "                \"var√∞\", \"fundu\", \"ger√∞u\", \"ger√∞i\", \"l√°ta\", \"halda\", \"bjuggu\", \"fann\", \"halda\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e05ea8",
   "metadata": {},
   "source": [
    "###  Loading POS annotations\n",
    "\n",
    "There is currently no official POS tagger for Old Norse in CLTK. There is however one that has been trained with **spaCy** on **Menota** data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "018c64c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tags = dict(NC=\"common noun\",\n",
    "     NP=\"proper noun\",\n",
    "     AJ=\"adjective\",\n",
    "     PE=\"personal pronoun\",\n",
    "     PI=\"indefinite pronoun\",\n",
    "     DP=\"possessive determiner\",\n",
    "     DD=\"demonstrative determiner\",\n",
    "     DQ=\"quantifier determiner\",\n",
    "     PD=\"pronoun/determiner\",\n",
    "     NA=\"cardinal determiner\",\n",
    "     NO=\"ordinal determiner\",\n",
    "     VB=\"verb\",\n",
    "     AV=\"adverb\",\n",
    "     AT=\"article\",\n",
    "     AP=\"preposition\",\n",
    "     CC=\"coordinating conjunction\",\n",
    "     CS=\"subordinating conjunction\",\n",
    "     IT=\"interjection\",\n",
    "     IM=\"infinitive marker\",\n",
    "     RP=\"relative particle\",\n",
    "     UA=\"unassigned\"\n",
    "    )\n",
    "\n",
    "informative_pos_tags = [\"NC\", \"NP\", \"AJ\", \"VB\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "19fe612e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open(\"annotated_pos_ESR.csv\", \"r\") as f:\n",
    "    pos_lines_ESR = [line.split(\"\\t\") for line in f.read().split(\"\\n\")]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4af174b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open(\"annotated_pos_GS.csv\", \"r\") as f:\n",
    "    pos_lines_GS = [line.split(\"\\t\") for line in f.read().split(\"\\n\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a566fb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNCTUATION = [\".\", \",\", \";\", \"!\", \"?\", \":\", \"\\\"\", \"'\", \"(\", \")\", \"[\", \"]\", \"{\", \"}\", \"--\", \".]\", \"-\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0865869c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', 'fr√°', 'au√∞i', 'dj√∫p√∫√∞gu', 'ok']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ESR_analysed_2.tokens[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03431af",
   "metadata": {},
   "source": [
    "### Aligning tags with text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ad4f9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_annotations(cltk_doc, spacy_annotations):\n",
    "    \"\"\"\n",
    "    Assign POS tags to words of a CLTK doc from spaCy annotations\n",
    "    \"\"\"\n",
    "    cltk_words = cltk_doc.words\n",
    "    i, j = 0, 0\n",
    "    m = len(cltk_words)\n",
    "    n = len(spacy_annotations)\n",
    "    while i < m and j < n:\n",
    "        spacy_token = spacy_annotations[j][0]\n",
    "        if not spacy_token or spacy_token in PUNCTUATION:\n",
    "            j += 1\n",
    "        elif cltk_words[i].string in PUNCTUATION:\n",
    "            i += 1\n",
    "        else:\n",
    "            #print(spacy_token.lower(), cltk_words[i].string)\n",
    "            if spacy_token.lower() == cltk_words[i].string:\n",
    "                \n",
    "                cltk_words[i].xpos = spacy_annotations[j][1]\n",
    "            else:\n",
    "                cltk_words[i].xpos = \"UA\"\n",
    "            i += 1\n",
    "            j += 1\n",
    "    \n",
    "    cltk_doc.words = cltk_words\n",
    "    return cltk_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7e657baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ESR_analysed_2 = align_annotations(ESR_analysed_2, pos_lines_ESR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "93093c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PE'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ESR_analysed_2.words[78].xpos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a1baa8",
   "metadata": {},
   "source": [
    "### Filtering words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a3f759df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f21e743b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kiw(cltk_word, pos_tags=None):\n",
    "    \n",
    "    if pos_tags is None:\n",
    "        pos_tags = informative_pos_tags\n",
    "    return not cltk_word.stop and cltk_word.string and\\\n",
    "        cltk_word.string not in custom_stop_list and\\\n",
    "        cltk_word.string not in common_verbs and\\\n",
    "        cltk_word.xpos in pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e707371d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('menn', 40),\n",
       " ('eir√≠kr', 29),\n",
       " ('ma√∞r', 24),\n",
       " ('√æorbj√∂rn', 24),\n",
       " ('karlsefni', 23),\n",
       " ('leifr', 22),\n",
       " ('landit', 17),\n",
       " ('vetr', 16),\n",
       " ('verit', 16),\n",
       " ('skip', 14),\n",
       " ('manna', 14),\n",
       " ('land', 13),\n",
       " ('b√≥ndi', 13),\n",
       " ('sonr', 12),\n",
       " ('kona', 12),\n",
       " ('sumar', 11),\n",
       " ('gu√∞r√≠√∞r', 11),\n",
       " ('ormr', 11),\n",
       " ('bjarni', 11),\n",
       " ('d√≥ttur', 10)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [(word.string,word.xpos) for word in ESR_analysed_2.words if kiw(word)]\n",
    "c_ESR = Counter([w[0] for w in a])\n",
    "c_ESR.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d96ddb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "GS_analysed_2 = align_annotations(GS_analysed_2, pos_lines_GS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "10d6ab37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('leifr', 33),\n",
       " ('skip', 30),\n",
       " ('land', 25),\n",
       " ('menn', 21),\n",
       " ('landit', 20),\n",
       " ('ma√∞r', 17),\n",
       " ('eir√≠kr', 16),\n",
       " ('skipi', 15),\n",
       " ('gu√∞r√≠√∞r', 15),\n",
       " ('manna', 14),\n",
       " ('√æorvaldr', 13),\n",
       " ('vetr', 13),\n",
       " ('gr√¶nlandi', 13),\n",
       " ('bjarni', 13),\n",
       " ('landi', 12),\n",
       " ('karlsefni', 12),\n",
       " ('mikil', 11),\n",
       " ('ba√∞', 11),\n",
       " ('sonr', 10),\n",
       " ('kona', 10)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = [(word.string,word.xpos) for word in GS_analysed_2.words if kiw(word)]\n",
    "c_GS = Counter([w[0] for w in b])\n",
    "c_GS.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60de841",
   "metadata": {},
   "source": [
    "## Datation of texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfa2ed2",
   "metadata": {},
   "source": [
    "One way to estimate the date of writing of a text is by using linguistic features. In [of/um partikkelen som dateringskriterium for Eddakvada, Leiv Olsen, *Gripla XXXI*, 2020](https://timarit.is/page/7340957), the author shows that the more recent is a text, the more there are occurrences of *of* and *um* particles.\n",
    "\n",
    "> The of/um particle can be used as a dating criterium only when the material for analysis is extensive.\n",
    "\n",
    "The paper's focus if on Eddic poems (i.e. poems in the Poetic Edda).\n",
    "\n",
    "However, *of* and *um* are not only particles (former pre-verbs, German and Dutch speakers can understand), they are also prepositions governing the accusative and dative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e547d6a4",
   "metadata": {},
   "source": [
    "### Dictionary lookup\n",
    "CLTK has some dictionaries available. Zo√´ga's dictionary is for Old Norse. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cfcb7eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I)\n",
      "prep.\n",
      "1) with dat. and acc., over = yfir (fara of fj√∂ll; sitja of bor√∞i); of time, = um; of haust or of haustum, in the autumn; of aptaninn, in the evening; of hr√≠√∞, for a while; of allt, always;\n",
      "2) with acc. of, about (bera vitni of e-t);\n",
      "3) in a casual sense, poet.; of sanna s√∂k, for a just cause, justly.\n",
      "II)\n",
      "an enclitic particle, chiefly placed before verbs; ek drykk of gat ens d√Ωra mja√∞ar, I got a draught of the precious mead.\n",
      "III)\n",
      "n.\n",
      "1) great quantity, number; of fj√°r, immensity of wealt\n"
     ]
    }
   ],
   "source": [
    "from cltk.lexicon.non import OldNorseZoegaLexicon\n",
    "\n",
    "non_dictionary = OldNorseZoegaLexicon()\n",
    "\n",
    "print(non_dictionary.lookup(\"of\")[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "06330dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "older umb, prep. with acc. and dat.\n",
      "I. with acc.\n",
      "1) around (sl√° hring um e-n);\n",
      "2) about, all over (h√°rit f√©ll um hana alla); um allar sveitir, all over the country; mikill um her√∞ar, large about the shoulders, broad-shouldered; liggja um akkeri, to ride at anchor;\n",
      "3) of proportion; margir voru um einn, many against one; um einn hest voru tveir menn, two men to each horse;\n",
      "4) round, past, beyond, with verbs denoting motion (sigla vestr um Bretland); leggja um skut √æessu skipi, to pass by this shi\n"
     ]
    }
   ],
   "source": [
    "print(non_dictionary.lookup(\"um\")[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b419b0c8",
   "metadata": {},
   "source": [
    "### Displaying contexts and preparing analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c6084d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 10 of 58 matches:\n",
      " var b√∫inn fylg√∞u √æeir styrr honum √∫t um eyjar eir√≠kr sag√∞i √æeim at hann √¶tla√∞\n",
      " sonr √∫lfs kr√°ku s√° er hann rak vestr um haf √æ√° er hann fann gunnbjarnarsker k\n",
      " eir√≠ksey n√¶r mi√∞ri inni eystri bygg√∞ um v√°rit eftir f√≥r hann til eir√≠ksfjar√∞a\n",
      "√≠ksey fyrir mynni eir√≠ksfjar√∞ar eftir um sumarit f√≥r hann til √≠slands ok kom s\n",
      " landit h√©ti vel eir√≠kr var √° √≠slandi um vetrinn en um sumarit eftir f√≥r hann \n",
      "el eir√≠kr var √° √≠slandi um vetrinn en um sumarit eftir f√≥r hann at byggja land\n",
      " eyrar er fa√∞ir hans haf√∞i brot siglt um v√°rit √æau t√≠√∞endi √æ√≥ttu bjarna mikil \n",
      " √æetta d√¶gr √°√∞r √æeir s√° land ok r√¶ddu um me√∞ s√©r hvat landi √æetta mun vera en \n",
      "ir√∞ma√∞r jarls ok f√≥r √∫t til gr√¶nlands um sumarit eftir var n√∫ mikil umr√¶√∞a um \n",
      " um sumarit eftir var n√∫ mikil umr√¶√∞a um landaleitan leifr sonr eir√≠ks rau√∞a √≥\n"
     ]
    }
   ],
   "source": [
    "# One of the dependency is NLTK\n",
    "from nltk.text import Text\n",
    "t = Text(GS_analysed_2.tokens)\n",
    "t.concordance(\"um\", lines=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8dbcdeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_context(cltk_words, token, window_size=5, limit=5):\n",
    "    window_size = 2\n",
    "    limit_index = 0\n",
    "    for k, word in enumerate(cltk_words):\n",
    "        if word.string == token:\n",
    "            if 0 < k - window_size and k + window_size + 1 < len(GS_analysed_2.words) and limit_index < limit:\n",
    "                limit_index += 1\n",
    "                #print(\" \".join([f\"{w.string}/{w.xpos}\" for w in GS_analysed_2.words[k-window_size:k+window_size+1]]))\n",
    "                print(\" \".join([f\"{w.string}\" for w in cltk_words[k-window_size:k+window_size+1]]))\n",
    "                print(\" \".join([f\"{w.xpos}\"+\" \"*(len(w.string)-2) for w in cltk_words[k-window_size:k+window_size+1]]))\n",
    "                print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9e7c3901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "honum √∫t um eyjar eir√≠kr\n",
      "PE    AV AP NC    NC    \n",
      "--------------------------------------------------\n",
      "rak vestr um haf √æ√°\n",
      "VB  AV    AP NC  AV\n",
      "--------------------------------------------------\n",
      "eystri bygg√∞ um v√°rit eftir\n",
      "AJ     NC    VB NC    VP   \n",
      "--------------------------------------------------\n",
      "eir√≠ksfjar√∞ar eftir um sumarit f√≥r\n",
      "NC            AV    AP NC      VB \n",
      "--------------------------------------------------\n",
      "√° √≠slandi um vetrinn en\n",
      "AP NC      AP NC      CC\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "display_context(GS_analysed_2.words, \"um\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3a77d212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hans hans of √≥v√≠√∞a kannat\n",
      "PE   PE   CU AJ    NC    \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "display_context(GS_analysed_2.words, \"of\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b77a4c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leitu√∞u hans um eyjarnar √æeir\n",
      "VB      PE   AP NC       AJ  \n",
      "--------------------------------------------------\n",
      "eir√≠ki √∫t um eyjarnar ok\n",
      "NC     AV AP NC       CC\n",
      "--------------------------------------------------\n",
      "rak vestr um haf ok\n",
      "VB  AV    AP NC  CC\n",
      "--------------------------------------------------\n",
      "eystri bygg√∞ um v√°rit eftir\n",
      "AJ     NC    VB NC    VP   \n",
      "--------------------------------------------------\n",
      "en eftir um sumarit f√≥r\n",
      "VB VP    AP NC      VB \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "display_context(ESR_analysed_2.words, \"um\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c1390281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dv√∂l√∞ust √æeir of stund ok\n",
      "VB       DD   AP NC    CC\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "display_context(ESR_analysed_2.words, \"of\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e35a07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7146586",
   "metadata": {},
   "source": [
    "By Cl√©ment Besnier, [www.clementbesnier.eu](https://www.clementbesnier.eu)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
